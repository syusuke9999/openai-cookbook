{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tiktokenを使ったトークンのカウント方法について\n",
    "\n",
    "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md)はOpenAIによる高速なオープンソース・トークナイザーです。\n",
    "\n",
    "テキスト列 (例: `\"tiktoken is great!\"`) とエンコーディング (例: `\"cl100k_base\"`) が与えられた場合、トークナイザーはテキスト列をトークンのリストに分割できます (例: `[\"t\", \"iik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
    "\n",
    "テキスト文字列のトークンへの分割は、GPTモデルがテキストをトークンの形で参照するために有用です。テキスト文字列の中にいくつのトークンがあるかを知ることで (a) 文字列が長すぎてテキストモデルが処理できないか、(b) OpenAIのAPI呼び出しがいくらかかるか（利用料金はトークン単位なので）がわかります。\n",
    "\n",
    "\n",
    "## エンコーディング\n",
    "\n",
    "エンコーディングは、テキストがトークンに変換される方法を指定します。異なるモデルは異なるエンコーディングを使用します。\n",
    "\n",
    "`tiktoken` は、OpenAI モデルで使用される3つのエンコーディングをサポートしています。\n",
    "\n",
    "| エンコーディング名                | OpenAI モデル                                         |\n",
    "|--------------------------|----------------------------------------------------|\n",
    "| `cl100k_base`            | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002` |\n",
    "| `p50k_base`              | Codex モデル、`text-davinci-002`、`text-davinci-003`    |\n",
    "| `r50k_base` (または `gpt2`) | `davinci` などの GPT-3 モデル                            |\n",
    "\n",
    "以下のように `tiktoken.encoding_for_model()` を使用して、モデルのエンコーディングを取得できます。\n",
    "\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "```\n",
    "\n",
    "`p50k_base` は `r50k_base` と大幅に重複しており、非コードのアプリケーションでは通常、同じトークンが得られます。\n",
    "\n",
    "## 言語別のトークナイザーライブラリ\n",
    "\n",
    "`cl100k_base` と `p50k_base` のエンコーディングについては、以下のライブラリがあります。\n",
    "\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
    "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken)\n",
    "- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\n",
    "\n",
    "`r50k_base` (`gpt2`) のエンコーディングについては、多くの言語でトークナイザーが利用可能です。\n",
    "\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (または [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
    "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
    "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
    "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
    "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
    "\n",
    "（OpenAI は、サードパーティのライブラリについて、いかなる保証も行いません。）\n",
    "\n",
    "## 一般的な文字列のトークナイズ方法\n",
    "\n",
    "英語では、トークンの長さは通常、1文字から1単語までの範囲です（例： `\"t\"` または `\" great\"`）。ただし、一部の言語では、トークンが1文字未満または1単語より長くなる場合があります。スペースは通常、単語の先頭とグループ化されます（例： `\" is\"` ではなく `\"is \"` または `\" \"`+`\"is\"`）。文字列がどのようにトークナイズされるかを簡単に確認できるのは、[OpenAI Tokenizer](https://beta.openai.com/tokenizer) です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. `tiktoken`のインストール\n",
    "\n",
    "必要に応じて、`pip`を使用して`tiktoken`をインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:24:20.235557900Z",
     "start_time": "2023-05-05T05:24:12.892559500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from tiktoken) (2.30.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from tiktoken) (2023.5.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openaiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from openai) (2.30.0)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-win_amd64.whl (319 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.20->openai) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yagi-\\dropbox\\pycharmprojects\\openai-cookbook\\venv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 tqdm-4.65.0 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `tiktoken`をインポートする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:24:46.999560600Z",
     "start_time": "2023-05-05T05:24:46.979560900Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. エンコーディングをロードする\n",
    "\n",
    "`tiktoken.get_encoding()` を使用して、名前でエンコーディングをロードします。\n",
    "\n",
    "初回実行時には、ダウンロードにインターネット接続が必要です。以降の実行では、インターネット接続は必要ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:01:15.326242Z",
     "start_time": "2023-05-05T05:01:12.319245500Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたモデル名に対して正しいエンコーディングを自動的に読み込むには、 `tiktoken.encoding_for_model()` を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:02:24.951467400Z",
     "start_time": "2023-05-05T05:02:24.936468700Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. encoding.encode()`でテキストをトークンに変換する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()` メソッドは、文字列をトークン整数のリストに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.encode(\"tiktokenは素晴らしい!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()` が返すリストの長さを数えて、トークンを数える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:04:40.246635200Z",
     "start_time": "2023-05-05T05:04:40.225638900Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"テキスト文字列中のトークンの数を返します。\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:05:00.586860300Z",
     "start_time": "2023-05-05T05:05:00.572866900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `encoding.decode()` でトークンをテキストに変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.decode()` は、トークン整数のリストを文字列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:06:10.525839200Z",
     "start_time": "2023-05-05T05:06:10.472843100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tiktoken is great!'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: `.decode()` は単一のトークンに適用できますが、utf-8の領域にないトークンに対しては非可逆的であることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一トークンの場合、`.decode_single_token_bytes()`は単一の整数のトークンを、それが表すバイト列に安全に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:08:19.439326400Z",
     "start_time": "2023-05-05T05:08:19.427324300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[b't', b'ik', b'token', b' is', b' great', b'!']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(文字列の前にある`b`はバイト文字列であることを示す)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. エンコーディングの比較\n",
    "\n",
    "エンコーディングの種類によって、単語の分割、スペースのグループ化、非英語文字の扱い方などが異なります。上記の方法を用いて、少数のサンプル文字列で異なるエンコーディングを比較することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:11:03.752288200Z",
     "start_time": "2023-05-05T05:11:03.725290300Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_encodings(example_string: str) -> None:\n",
    "    \"\"\"3つの文字列のエンコーディングを比較した結果を表示します。\"\"\"\n",
    "    # サンプル文字列を表示する\n",
    "    print(f'\\nExample string: \"{example_string}\"')\n",
    "    # それぞれのエンコーディング形式に対して、# トークン数、トークン整数、トークンバイト列を表示する\n",
    "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(example_string)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:11:12.095290400Z",
     "start_time": "2023-05-05T05:11:06.920288500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"antidisestablishmentarianism\"\n",
      "\n",
      "gpt2: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "cl100k_base: 6 tokens\n",
      "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
      "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:11:18.718289800Z",
     "start_time": "2023-05-05T05:11:18.688291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"2 + 2 = 4\"\n",
      "\n",
      "gpt2: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "cl100k_base: 7 tokens\n",
      "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
      "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"2 + 2 = 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:11:23.593290900Z",
     "start_time": "2023-05-05T05:11:23.576290400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"お誕生日おめでとう\"\n",
      "\n",
      "gpt2: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_base: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "cl100k_base: 9 tokens\n",
      "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"お誕生日おめでとう\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. チャットAPIの呼び出しでトークンを数える\n",
    "\n",
    "ChatGPTモデル（`gpt-3.5-turbo`や`gpt-4`など）は、古い補完モデルと同じようにトークンを使用しますが、メッセージベースのフォーマットのため、会話で使用されるトークンの数を数えることがより困難になっています。\n",
    "\n",
    "以下は、`gpt-3.5-turbo-0301`または`gpt-4-0314`に渡されたメッセージのトークンをカウントするためのサンプル関数です。\n",
    "\n",
    "メッセージからトークンをカウントする正確な方法は、モデルによって変化する可能性があることに注意してください。以下の関数によるカウントは推定値であり、時代を超えて保証されるものではないとお考えください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:22:23.859558Z",
     "start_time": "2023-05-05T05:22:23.843559400Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"メッセージのリストで使用されるトークンの数を返します。\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"警告：モデルが見つかりません。cl100k_baseエンコーディングを使用しています。\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"警告：gpt-3.5-turboは時間の経過とともに変更される可能性があります。gpt-3.5-turbo-0301を仮定してトークン数を返します。\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"警告：gpt-4は時間の経過とともに変更される可能性があります。gpt-4-0314を仮定してトークン数を返します。\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # 各メッセージは<|start|>{role/name}\\n{content}<|end|>\\nに従います。\n",
    "        tokens_per_name = -1  # 名前がある場合、roleは省略されます。\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages()は、モデル{model}に対して実装されていません。メッセージがトークンに変換される方法については、https://github.com/openai/openai-python/blob/main/chatml.mdを参照してください。\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # 各返信は<|start|>assistant<|message|>でプライムされます。\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T05:25:44.067645300Z",
     "start_time": "2023-05-05T05:25:44.032645900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 上記の関数がOpenAI APIのレスポンスと一致することを確認しましょう。\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "127 prompt tokens counted by num_tokens_from_messages().\n",
      "127 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0314\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "    # 上記で定義された関数によるサンプルトークン数\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # OpenAI APIからのトークン数の例\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1  # ここでは入力トークンのみをカウントしているので、出力に無駄なトークンを使わないようにしましょう\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T05:30:06.603582900Z",
     "start_time": "2023-05-05T05:30:05.084585700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
